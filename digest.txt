Directory structure:
└── kokoro-runpod-tts/
    ├── Dockerfile
    ├── handler.py
    ├── requirements.txt
    ├── test_client.py
    └── .github/
        └── workflows/
            └── docker-build.yml

================================================
File: Dockerfile
================================================
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    espeak-ng \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Install Python packages
RUN pip install --no-cache-dir -r requirements.txt

# Copy handler
COPY handler.py .

# Pre-download models during build (optional - adds ~2GB to image)
# RUN python -c "from kokoro import KPipeline; KPipeline(lang_code='a'); KPipeline(lang_code='b')"

# Expose WebSocket port
EXPOSE 8000

# Run the handler
CMD ["python", "-u", "handler.py"]









================================================
File: handler.py
================================================
#!/usr/bin/env python3
"""
Kokoro TTS RunPod Handler with ElevenLabs WebSocket Compatibility
Supports:
1. RunPod REST API with streaming
2. ElevenLabs-compatible WebSocket server for Pipecat
"""

import asyncio
import json
import base64
import time
import numpy as np
from typing import Dict, Generator, Any, Optional
import os
import sys
import logging
import threading
from dataclasses import dataclass
from collections import defaultdict

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import required libraries
import torch
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import runpod

# Force single-threaded for consistent performance
torch.set_num_threads(1)

# Import Kokoro
try:
    from kokoro import KPipeline
except ImportError:
    logger.error("Kokoro not installed. Please install with: pip install kokoro>=0.9.4")
    sys.exit(1)

# Global pipelines - loaded once at container start
PIPELINES = {}
LOAD_START_TIME = time.time()

@dataclass
class AudioContext:
    """Tracks audio generation context"""
    context_id: str
    text_buffer: str = ""
    audio_chunks: list = None
    word_times: list = None
    cumulative_time: float = 0.0
    
    def __post_init__(self):
        if self.audio_chunks is None:
            self.audio_chunks = []
        if self.word_times is None:
            self.word_times = []

def initialize_pipelines():
    """Pre-load all language models for instant access"""
    global PIPELINES
    
    languages = {
        'a': 'American English',
        'b': 'British English',
    }
    
    logger.info("Pre-loading Kokoro models...")
    
    for lang_code, lang_name in languages.items():
        try:
            start = time.time()
            pipeline = KPipeline(lang_code=lang_code)
            
            # Warm up with dummy inference
            logger.info(f"Warming up {lang_name} model...")
            list(pipeline("test", voice='af_bella'))
            
            PIPELINES[lang_code] = pipeline
            logger.info(f"Loaded {lang_name} in {time.time() - start:.2f}s")
            
        except Exception as e:
            logger.error(f"Failed to load {lang_name}: {e}")
    
    total_time = time.time() - LOAD_START_TIME
    logger.info(f"All models loaded in {total_time:.2f}s")

# Initialize pipelines on import
initialize_pipelines()

# Create FastAPI app for WebSocket server
app = FastAPI(title="Kokoro TTS WebSocket Server")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def calculate_word_times(text: str, audio_duration: float, cumulative_time: float = 0) -> list:
    """Calculate word timestamps based on text and audio duration"""
    words = text.split()
    if not words:
        return []
    
    # Simple linear distribution of words across audio duration
    word_duration = audio_duration / len(words)
    word_times = []
    
    for i, word in enumerate(words):
        timestamp = cumulative_time + (i * word_duration)
        word_times.append((word, timestamp))
    
    return word_times

def create_alignment_data(text: str, audio_duration_ms: float) -> dict:
    """Create ElevenLabs-compatible alignment data"""
    chars = list(text)
    if not chars:
        return {"chars": [], "charStartTimesMs": [], "charsDurationsMs": []}
    
    char_duration_ms = audio_duration_ms / len(chars) if chars else 0
    
    return {
        "chars": chars,
        "charStartTimesMs": [int(i * char_duration_ms) for i in range(len(chars))],
        "charsDurationsMs": [int(char_duration_ms) for _ in chars]
    }

@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "status": "ready",
        "models_loaded": list(PIPELINES.keys()),
        "load_time": f"{time.time() - LOAD_START_TIME:.2f}s",
        "mode": "elevenlabs-compatible"
    }

@app.websocket("/v1/text-to-speech/{voice_id}/multi-stream-input")
async def websocket_elevenlabs_compatible(
    websocket: WebSocket,
    voice_id: str,
    model_id: str = "eleven_flash_v2_5",
    output_format: str = "pcm_16000",
    language_code: Optional[str] = None,
    auto_mode: str = "true",
    enable_ssml_parsing: Optional[bool] = None,
    enable_logging: Optional[bool] = None,
):
    """ElevenLabs-compatible WebSocket endpoint for Pipecat"""
    await websocket.accept()
    
    logger.info(f"ElevenLabs-compatible WebSocket connected: voice={voice_id}, model={model_id}")
    
    # Track contexts for this connection
    contexts: Dict[str, AudioContext] = {}
    active_context_id: Optional[str] = None
    
    try:
        while True:
            # Receive message
            message = await websocket.receive_text()
            
            try:
                data = json.loads(message)
            except json.JSONDecodeError:
                # Handle keepalive empty messages
                continue
            
            # Handle different message types
            if not data:
                # Keepalive message
                continue
                
            if data.get("close_socket"):
                logger.info("Close socket requested")
                break
                
            if data.get("close_context"):
                context_id = data.get("context_id")
                if context_id in contexts:
                    logger.info(f"Closing context {context_id}")
                    del contexts[context_id]
                    if active_context_id == context_id:
                        active_context_id = None
                continue
            
            # Handle text generation
            text = data.get("text", "").strip()
            context_id = data.get("context_id")
            
            if not context_id:
                logger.warning("No context_id provided, skipping")
                continue
                
            # Handle initial space for new context
            if text == " " and context_id not in contexts:
                # Initialize new context
                contexts[context_id] = AudioContext(context_id=context_id)
                active_context_id = context_id
                logger.info(f"Initialized new context {context_id}")
                continue
            
            if not text or text == " ":
                continue
                
            # Get or create context
            if context_id not in contexts:
                contexts[context_id] = AudioContext(context_id=context_id)
            
            context = contexts[context_id]
            context.text_buffer = text
            
            # Get voice settings if provided
            voice_settings = data.get("voice_settings", {})
            speed = voice_settings.get("speed", 1.0)
            
            # Determine language/pipeline
            lang_code = voice_id[0] if voice_id and voice_id[0] in PIPELINES else 'a'
            pipeline = PIPELINES.get(lang_code, PIPELINES['a'])
            
            logger.info(f"Generating audio for context {context_id}: '{text}'")
            
            # Generate audio
            generation_start = time.time()
            audio_chunks = []
            total_samples = 0
            
            try:
                for graphemes, phonemes, audio in pipeline(text, voice=voice_id, speed=speed):
                    # Convert to PCM16
                    audio_pcm = (audio * 32767).astype(np.int16)
                    audio_bytes = audio_pcm.tobytes()
                    audio_chunks.append(audio)
                    total_samples += len(audio)
                    
                    # Send audio chunk immediately
                    chunk_message = {
                        "audio": base64.b64encode(audio_bytes).decode('utf-8'),
                        "contextId": context_id,
                        "isFinal": False
                    }
                    await websocket.send_text(json.dumps(chunk_message))
                
                # Calculate audio duration and create alignment
                if audio_chunks:
                    full_audio = np.concatenate(audio_chunks)
                    audio_duration = len(full_audio) / 24000.0  # 24kHz sample rate
                    audio_duration_ms = audio_duration * 1000
                    
                    # Send alignment data
                    alignment = create_alignment_data(text, audio_duration_ms)
                    alignment_message = {
                        "alignment": alignment,
                        "contextId": context_id
                    }
                    await websocket.send_text(json.dumps(alignment_message))
                    
                    # Update context cumulative time
                    context.cumulative_time += audio_duration
                
                # Send final message
                generation_time = time.time() - generation_start
                final_message = {
                    "isFinal": True,
                    "contextId": context_id,
                    "metadata": {
                        "chunks": len(audio_chunks),
                        "characters": len(text),
                        "generation_time_ms": int(generation_time * 1000),
                        "audio_duration_ms": int(audio_duration_ms) if audio_chunks else 0
                    }
                }
                await websocket.send_text(json.dumps(final_message))
                
                logger.info(f"Completed generation for context {context_id} in {generation_time:.2f}s")
                
            except Exception as e:
                logger.error(f"Error generating audio: {e}")
                error_message = {
                    "error": str(e),
                    "contextId": context_id
                }
                await websocket.send_text(json.dumps(error_message))
    
    except WebSocketDisconnect:
        logger.info("WebSocket disconnected by client")
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
    finally:
        logger.info(f"WebSocket connection closed. Had {len(contexts)} contexts.")

# RunPod streaming handler
def generator_handler(job: Dict[str, Any]) -> Generator[Dict[str, Any], None, None]:
    """
    Generator handler for streaming TTS output via RunPod
    """
    job_start = time.time()
    
    try:
        job_input = job["input"]
        text = job_input.get("text", "")
        voice_id = job_input.get("voice_id", "af_bella")
        speed = job_input.get("speed", 1.0)
        output_format = job_input.get("output_format", "pcm_16000")
        streaming = job_input.get("streaming", True)
        
        if not text:
            yield {"error": "No text provided"}
            return
        
        # Get pipeline
        lang_code = voice_id[0] if voice_id else 'a'
        pipeline = PIPELINES.get(lang_code, PIPELINES['a'])
        
        # Send initial status
        yield {
            "status": "started",
            "text_info": {
                "text": text,
                "voice_id": voice_id,
                "characters": len(text),
            }
        }
        
        # Generate audio with streaming
        audio_chunks = []
        chunk_count = 0
        total_samples = 0
        first_chunk_time = None
        
        for graphemes, phonemes, audio in pipeline(text, voice=voice_id, speed=speed):
            if first_chunk_time is None:
                first_chunk_time = time.time() - job_start
            
            # Convert to PCM16
            audio_pcm = (audio * 32767).astype(np.int16)
            audio_bytes = audio_pcm.tobytes()
            audio_chunks.append(audio)
            
            chunk_count += 1
            total_samples += len(audio)
            
            # Stream each chunk
            if streaming:
                yield {
                    "audio_chunk": base64.b64encode(audio_bytes).decode('utf-8'),
                    "chunk_number": chunk_count,
                    "chunk_samples": len(audio),
                    "total_samples": total_samples,
                    "sample_rate": 24000,
                    "format": output_format,
                    "timestamp": time.time() - job_start
                }
        
        # Final result
        if audio_chunks:
            full_audio = np.concatenate(audio_chunks)
            audio_duration = len(full_audio) / 24000.0
            
            # Encode full audio
            if "pcm" in output_format:
                full_audio_data = (full_audio * 32767).astype(np.int16).tobytes()
            else:
                import io
                import soundfile as sf
                buffer = io.BytesIO()
                sf.write(buffer, full_audio, 24000, format='wav')
                full_audio_data = buffer.getvalue()
            
            processing_time = time.time() - job_start
            
            yield {
                "status": "completed",
                "audio": base64.b64encode(full_audio_data).decode('utf-8'),
                "format": output_format,
                "sample_rate": 24000,
                "total_chunks": chunk_count,
                "audio_duration": audio_duration,
                "processing_time": processing_time,
                "first_chunk_latency": first_chunk_time,
                "characters": len(text),
                "real_time_factor": processing_time / audio_duration if audio_duration > 0 else 0
            }
        
    except Exception as e:
        logger.error(f"Generator handler error: {e}")
        yield {"error": str(e), "status": "failed"}

# Start servers based on environment
if __name__ == "__main__":
    if os.environ.get("RUNPOD_POD_ID"):
        logger.info("Starting in RunPod mode with WebSocket server")
        
        # Start WebSocket server in background thread
        def run_websocket_server():
            uvicorn.run(
                app,
                host="0.0.0.0",
                port=8000,
                loop="uvloop",
                log_level="info"
            )
        
        websocket_thread = threading.Thread(target=run_websocket_server, daemon=True)
        websocket_thread.start()
        logger.info("ElevenLabs-compatible WebSocket server started on port 8000")
        
        # Start RunPod handler
        runpod.serverless.start({
            "handler": generator_handler,
            "return_aggregate_stream": True
        })
    else:
        # Local testing mode - just run WebSocket server
        logger.info("Starting in local mode (WebSocket only)")
        uvicorn.run(app, host="0.0.0.0", port=8000)


================================================
File: requirements.txt
================================================
runpod==1.6.2
kokoro>=0.9.4
soundfile==0.12.1
numpy==1.24.3
torch==2.1.0
fastapi==0.104.1
uvicorn[standard]==0.24.0
websockets==12.0
pydantic==2.5.0


================================================
File: test_client.py
================================================



================================================
File: .github/workflows/docker-build.yml
================================================
name: Build and Push Docker Image

on:
  push:
    branches: [ main ]
  workflow_dispatch:  # Allow manual trigger

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Login to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_TOKEN }}
    
    - name: Generate unique tag
      id: tags
      run: |
        # Generate timestamp-based tag
        TIMESTAMP=$(date +%Y%m%d-%H%M%S)
        
        # Get short SHA
        SHORT_SHA=$(echo ${GITHUB_SHA} | cut -c1-7)
        
        # Get branch name (replace / with -)
        BRANCH_NAME=$(echo ${GITHUB_REF#refs/heads/} | sed 's/\//-/g')
        
        # Create multiple tag variants
        echo "timestamp=${TIMESTAMP}" >> $GITHUB_OUTPUT
        echo "sha=${SHORT_SHA}" >> $GITHUB_OUTPUT
        echo "branch=${BRANCH_NAME}" >> $GITHUB_OUTPUT
        
        # Combined tags
        echo "version=${BRANCH_NAME}-${TIMESTAMP}-${SHORT_SHA}" >> $GITHUB_OUTPUT
        echo "simple=${TIMESTAMP}-${SHORT_SHA}" >> $GITHUB_OUTPUT
        echo "date=$(date +%Y%m%d)" >> $GITHUB_OUTPUT
    
    - name: Build and push
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: |
          ${{ secrets.DOCKER_USERNAME }}/kokoro-tts:latest
          ${{ secrets.DOCKER_USERNAME }}/kokoro-tts:${{ steps.tags.outputs.version }}
          ${{ secrets.DOCKER_USERNAME }}/kokoro-tts:${{ steps.tags.outputs.simple }}
          ${{ secrets.DOCKER_USERNAME }}/kokoro-tts:build-${{ github.run_number }}
          ${{ secrets.DOCKER_USERNAME }}/kokoro-tts:sha-${{ steps.tags.outputs.sha }}
          ${{ secrets.DOCKER_USERNAME }}/kokoro-tts:${{ steps.tags.outputs.date }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

